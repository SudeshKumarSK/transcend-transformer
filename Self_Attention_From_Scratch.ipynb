{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TabHuxvzj4R-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the query, key and value vectors and their dimensions. In the \"Attention is All you need\" paper, the actual dimensions of Wq, Wk and Wv matrices are, 512 x 64 becasue the I/P embeddings vectors are of dimension 512. So, if we have 4 words, I/P will have a dimension of 4 x 512. After these Self Attention layers, expected dimensions of the query, key and value vectors are 1 x 64. Hence, for all the input words together in the form of a O/P matrix, it will be 4 X 64\n",
        "\n",
        "### For understanding the mechanism, we do not consider the matrices WQ, WK and WV. Rather we directly randomly initialize the query, key and value vectors in the form of 1 X 8 dimension. Since, we have 4 words in the I/P sentence, it these will look like 4 x 8 matrices."
      ],
      "metadata": {
        "id": "WEZldJpdLyeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = 4  # Number of words in the I/P sentence\n",
        "\n",
        "# Dimensions of the vectors\n",
        "d_k = 8\n",
        "d_v = 8\n",
        "\n",
        "# Random Initialization of vectors\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ],
      "metadata": {
        "id": "TUkVE7AiNrMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbq1HQPiN3VF",
        "outputId": "2d693ce5-77da-41f2-bd0b-4827f849b48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.73295046, -1.01856123, -0.05692238,  0.84549785,  0.73518234,\n",
              "         0.28877697, -0.68057301,  1.60372692],\n",
              "       [-0.59850236,  1.31942612,  1.07497577,  0.27911664, -1.43302758,\n",
              "         0.63542193, -0.11069461, -0.46895461],\n",
              "       [-2.51227147,  1.28685185,  0.41192541, -0.40138432, -1.1682551 ,\n",
              "        -0.99897481, -1.67014039, -1.33189188],\n",
              "       [-0.4541441 ,  0.49158284, -1.07186903, -0.36634175,  0.38987809,\n",
              "        -0.80854428,  0.33013949,  0.80924413]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrSTNz7bOGCo",
        "outputId": "1ab7fbed-9b42-4143-9442-a501f9f362d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.79680821, -1.00435721, -0.08761221, -0.17929451, -0.61332812,\n",
              "        -2.03886942,  0.68464872, -0.21430272],\n",
              "       [ 1.05226059, -0.18622262,  0.7634342 ,  0.56260192,  0.07880734,\n",
              "        -0.57725068,  0.3289039 ,  1.33388147],\n",
              "       [-0.16095804, -0.09937703,  0.06683818, -0.79259057,  0.6560552 ,\n",
              "         0.45312437,  0.77328347,  0.74865733],\n",
              "       [ 1.00119153,  1.76478707,  0.15744213, -1.33803559, -1.58144575,\n",
              "        -1.76573614,  0.5914121 ,  0.01702544]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwbaRzh2OHaR",
        "outputId": "9ec388ce-6d66-4625-aa0f-331c6a12c8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.24141684, -2.17324842,  0.42929517, -1.64532319,  0.65945414,\n",
              "         0.13581085,  2.5898868 , -1.92892245],\n",
              "       [-0.21441448, -1.52829474,  0.60023029,  1.00615942,  0.99322276,\n",
              "         0.85164205,  0.38279799, -1.13031028],\n",
              "       [-1.05493284, -0.23369413,  1.29379467, -1.2126394 ,  0.32959178,\n",
              "        -1.23363966, -1.06027237, -0.46822239],\n",
              "       [ 0.35938068, -1.09129034,  1.23356365, -0.51658256, -1.06698389,\n",
              "         0.16209556, -0.41461667, -0.15328363]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention Mechanism\n",
        "\n",
        "### Huge crux of Attention is the matrix multiplication of query vector and key vector. It rolls out to be the multiplication of \"What I want?\" and \"What I can offer?\""
      ],
      "metadata": {
        "id": "IStZxTRWOSrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q @ k.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3rD2pO-OW2p",
        "outputId": "b07bd065-a6f2-4bd3-aa70-cb0a2c2bebe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.38890882,  1.65722606,  0.83279578, -5.71938555],\n",
              "       [-1.38442352, -1.0394512 , -1.27306734,  2.59588676],\n",
              "       [ 2.64048738, -4.6358566 , -2.8855697 ,  2.95871622],\n",
              "       [ 1.48973224,  0.09165008,  0.99351174,  1.75440582]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling these value by square root of d_k to bring these values to have a variance of ~1"
      ],
      "metadata": {
        "id": "37WyyJvkPF4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = q @ k.T / math.sqrt(d_k)"
      ],
      "metadata": {
        "id": "E-FM9uqMO6JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apsn3e8VPaSY",
        "outputId": "f38f2af9-1c63-42ff-b957-282ea77e296b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.13750003,  0.58591789,  0.29443777, -2.02210815],\n",
              "       [-0.48946763, -0.3675015 , -0.45009727,  0.91778457],\n",
              "       [ 0.93355327, -1.63902282, -1.02020295,  1.04606415],\n",
              "       [ 0.52669988,  0.0324032 ,  0.35125944,  0.62027613]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking for the decoder!"
      ],
      "metadata": {
        "id": "B2re4jZvPhjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones( (L, L) ))"
      ],
      "metadata": {
        "id": "06ofGQnWPkLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aNdjpAnPq6h",
        "outputId": "7c71c0e2-56da-4f7f-f277-f0d386852355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ],
      "metadata": {
        "id": "P3mqHC_9PrZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjl5SkUIP0yB",
        "outputId": "d75bf805-9601-4ae6-d5c1-19960c589d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled + mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T6cp3DCP14p",
        "outputId": "fdc684e5-5e25-4855-91e5-864ec7b39af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.13750003,        -inf,        -inf,        -inf],\n",
              "       [-0.48946763, -0.3675015 ,        -inf,        -inf],\n",
              "       [ 0.93355327, -1.63902282, -1.02020295,        -inf],\n",
              "       [ 0.52669988,  0.0324032 ,  0.35125944,  0.62027613]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis = 1)).T"
      ],
      "metadata": {
        "id": "FwQxsfW3P6Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaledDotProductAttention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "\n",
        "  return out, attention"
      ],
      "metadata": {
        "id": "E-ufpRe-WBKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaledDotProductAttention(q, k, v, mask=None)"
      ],
      "metadata": {
        "id": "XsfJUrL7dnSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw60b7PudwiK",
        "outputId": "3696fd50-78aa-4b6c-8a0e-8950b166b190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.24141684, -2.17324842,  0.42929517, -1.64532319,  0.65945414,\n",
              "         0.13581085,  2.5898868 , -1.92892245],\n",
              "       [-0.22709334, -1.83113029,  0.51996835, -0.23883418,  0.83650297,\n",
              "         0.51552623,  1.41912818, -1.50529559],\n",
              "       [-0.3343886 , -1.90713389,  0.54060457, -1.42880243,  0.64198774,\n",
              "         0.02131811,  2.02681827, -1.70889937],\n",
              "       [-0.24322313, -1.2685952 ,  0.91216408, -0.7375808 ,  0.1043722 ,\n",
              "        -0.0568967 ,  0.41679405, -0.89637671]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDDFi8x4dz-q",
        "outputId": "ea762e5f-8f62-4870-8bcd-93de847e44b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.46954621, 0.53045379, 0.        , 0.        ],\n",
              "       [0.82096461, 0.06267132, 0.11636407, 0.        ],\n",
              "       [0.28191397, 0.17196746, 0.23655045, 0.30956813]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T05ZW5UaeB5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}